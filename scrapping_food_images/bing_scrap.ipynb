{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bing scrap.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapping Images with Bing\n",
        "## Install bing-image-downloader\n",
        "To gather food images dataset, we use [bing-image-downloader](https://pypi.org/project/bing-image-downloader/) library. So, the first step is to install the library."
      ],
      "metadata": {
        "id": "dTrQD3q28Yb1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aX9YhDhU46F",
        "outputId": "b0179600-1a26-4490-f1ea-addc7a572912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bing-image-downloader\n",
            "  Downloading bing_image_downloader-1.1.2-py3-none-any.whl (5.9 kB)\n",
            "Installing collected packages: bing-image-downloader\n",
            "Successfully installed bing-image-downloader-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install bing-image-downloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir food"
      ],
      "metadata": {
        "id": "LOEG0IXUU8vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bing_image_downloader import downloader\n",
        "import IPython\n",
        "import os"
      ],
      "metadata": {
        "id": "8UzZI8CYU9Tp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we use Google Colab, we can mount our Google Drives to save files, so there's no need to download resulted images."
      ],
      "metadata": {
        "id": "byHvV12IAVO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ceOsrboVFNQ",
        "outputId": "f9ffabb3-c145-4cc1-f526-87f8a0a11503"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Downloader Function \n",
        "To make it easy to download several type of food images and automating it, we define this helper function."
      ],
      "metadata": {
        "id": "1oVD47UIA4HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_bulk_images(list_name, dir, each_limit=200):\n",
        "  \"\"\"Download list of images from bing images search engine.\"\"\"\n",
        "  exists = os.listdir(dir)\n",
        "\n",
        "  for name in list_name:\n",
        "    if name not in exists:\n",
        "      downloader.download (name,limit=each_limit, output_dir=dir, \n",
        "                          adult_filter_off=True)  \n",
        "      IPython.display.clear_output()\n",
        "  \n",
        "  for name in list_name:\n",
        "    print(f\"{len(os.listdir(dir+'/'+name))} images downloaded.\")"
      ],
      "metadata": {
        "id": "6TLx6AKzU_TO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "makanan = [\n",
        "           'mie goreng', 'pempek', 'nasi goreng', 'gado-gado', 'rawon', 'bebek betutu',\n",
        "           'sate', 'bakso', 'gudeg', 'rendang', 'kimchi', 'tteokbokki', 'hotteok', 'oden',\n",
        "           'jjangmyeon', 'kimbab', 'sushi', 'ramen', 'udon', 'tamagoyaki', 'tofu', 'onigiri',\n",
        "           'tempura', 'yakitori', 'biryani', 'jalebi', 'samosa', 'laddu', 'kulfi', 'pizza', \n",
        "           'bakpao', 'capcay', 'siomay', 'fu yung hai', 'roti maryam', 'kebab', 'sambosa',\n",
        "           'musakhan', 'bubur ayam', 'aseeda', 'tiramisu', 'kue keranjang', 'nasi hainam',\n",
        "           'manggo sticky rice', 'kerak telor', 'lumpia', 'nasi padang', 'soto', 'mandu'\n",
        "           ]"
      ],
      "metadata": {
        "id": "M7QywX7CVHJu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set directory to save images. In this case, the image is directly saved to [Google Drive directory](https://drive.google.com/drive/folders/1o99sJK-AYufm2p4U4Zpgq71jae-dAVb1?usp=sharing)."
      ],
      "metadata": {
        "id": "jIsCU2tIBhGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_dir = 'drive/MyDrive/Capstone Bangkit/ML/food'"
      ],
      "metadata": {
        "id": "Rc8sEvBgJbtv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_bulk_images(makanan, drive_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCUqQp7WVJOd",
        "outputId": "b6acce5d-264a-4f70-e009-a297f01e5434"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "214 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n",
            "200 images downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Dataset\n",
        "After finished scrapping images, we clean it manually trough [Google Drive](https://drive.google.com/drive/folders/1o99sJK-AYufm2p4U4Zpgq71jae-dAVb1?usp=sharing). Then we archive the dataset into zipfile to make it easier to load the data next time."
      ],
      "metadata": {
        "id": "BSmV9AZrCiie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive(drive_dir+'/../food', 'zip', drive_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "XuyTA8AsJEHN",
        "outputId": "a7c5084c-8da7-4420-9a3a-af70b1ecacd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Capstone Bangkit/ML/foods.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}